{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wiki text Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #vectorization\n",
    "import random #generate probability distribution \n",
    "import tensorflow as tf #ml\n",
    "import datetime #clock training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     C:\\Users\\69785hsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import webtext \n",
    "nltk.download('webtext')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('wiki.test.raw', encoding=\"utf8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text length in number of characters: 1288556\n"
     ]
    }
   ],
   "source": [
    "print('text length in number of characters:', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map chars to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars:  259\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print('total chars: ', len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split up into subsequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 429506\n"
     ]
    }
   ],
   "source": [
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' \\n = Robert Boulter = \\n \\n Robert Boulter', '= Robert Boulter = \\n \\n Robert Boulter is', 'obert Boulter = \\n \\n Robert Boulter is an']\n",
      "[' ', ' ', ' ']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[:3])\n",
    "print(next_chars[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[False  True False ... False False False]\n",
      "  [ True False False ... False False False]\n",
      "  [False  True False ... False False False]\n",
      "  ...\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]]\n",
      "\n",
      " [[False False False ... False False False]\n",
      "  [False  True False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  ...\n",
      "  [False  True False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]]\n",
      "\n",
      " [[False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  ...\n",
      "  [False  True False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]]]\n",
      "[[False  True False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False]\n",
      " [False  True False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False]\n",
      " [False  True False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False]]\n"
     ]
    }
   ],
   "source": [
    "print(x[:3])\n",
    "print(y[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filepath = \"wiki.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss',\n",
    "                             verbose=1, save_best_only=True,\n",
    "                             mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,\n",
    "                              patience=1, min_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [print_callback, checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/5\n",
      "429506/429506 [==============================] - 227s 530us/step - loss: 1.9426\n",
      "\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" Jaxon Benge and original bassist Mark Y\"\n",
      " Jaxon Benge and original bassist Mark Yout and and an the contract of the former to the controlled the strate of the state the state in the sens of the Roman and the Nero , and the state an an an an an an the strate of the contil state the since and the contralt and a state to be the controlled the controlled the strate to the compare to the side to the state to the since the first an an an the contralt and the second of the contralt t\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" Jaxon Benge and original bassist Mark Y\"\n",
      " Jaxon Benge and original bassist Mark Yout and in Charaction the straige an an the Merican state to intersifiely , they the first their intersing the central . The destrate on the reast of the Sout Helow , and an the are were not into the First an an the sent . \n",
      " Wan , leavy in the contraped the formen the seast to the relation . \n",
      " = = = = \n",
      " \n",
      " Antractians and the continue the near in the All @-@ Signal Rome Street and land to between t\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" Jaxon Benge and original bassist Mark Y\"\n",
      " Jaxon Benge and original bassist Mark Yaguntist gramplen scon roat from 154 . peam Gragoans of the Fapot at leatured contrant side tear . \n",
      " While Day , and be nat ano stormed to 1. Warthew maching . \n",
      " \n",
      " = = = = = = = \n",
      " \n",
      " The Lest Back sinched showed to Giza 1922 , strest tring the \n",
      " = = = = \n",
      " \n",
      " Signancy erais before to dieattour angoopied it Song in petulation Domitus B bednanke and zime the later is gavione than syter line the heldere\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" Jaxon Benge and original bassist Mark Y\"\n",
      " Jaxon Benge and original bassist Mark Yorth . Best \" stone by @-@ gop is band tmactocson zone pass alt sethorned stell med tropical Ravaeses Wbegues @-@ orner , the @-@ uprels poct ann twaveten to cross Bintulu d in PrMion Flok is lowdenter metedorice Recory In Some 1 , who hidful spolled his tropicationating crogrmenned dieclad now aiscary acories xodmen to , engurivulmss used gronga @-@ . an Empil dryence . His tornat  . \n",
      " Wall Wolld\n",
      "\n",
      "Epoch 00001: loss improved from inf to 1.94261, saving model to wiki.hdf5\n",
      "Epoch 2/5\n",
      "429506/429506 [==============================] - 224s 522us/step - loss: 1.6579\n",
      "\n",
      "----- Generating text after Epoch: 1\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"stricts . The Bintulu central police sta\"\n",
      "stricts . The Bintulu central police started the first and the first the companion and the armour the started the armour the started the first the first survived the show and the first review and a complex the first a the a complex of the first and the position , and the first a commission and the first had a the companion and the started the first the companion and the first and the provided the service and a the first reveloced the co\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"stricts . The Bintulu central police sta\"\n",
      "stricts . The Bintulu central police start was a streck . The armour . The seen the first developed the new and the popal of the olding the first complex same reason that and with Navy in the film on 1 @,@ 0000 . It begans and a fully the Eddie . Italian with the complex body and the nan as a formation division of the mitleen began began proved and the next in the armour the force ships of the Rart formier commerolly and a mer @-@ most \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"stricts . The Bintulu central police sta\"\n",
      "stricts . The Bintulu central police stalent 0hen . JuRimen nettones has sdyperonieved not planch ( 1005 kt \" . was wrestlers \" . Threghomea advalteng . , this season . He and was ab the Park 26r . . Bintermaty Replace atrapred a runs Pale jungened the 27 , freattherp Ausral Agariaval Jycusts and ftwarding these had hand @-@ day and astemparicls of their rew review fordd . The given on Provatia they assove ; they basibnes . The under Te\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"stricts . The Bintulu central police sta\"\n",
      "stricts . The Bintulu central police started fooricated as knote erths and by perenpines he SAlfte not hipaedships omus 0,ques Status Loxet unstrike 5l. Tibute . rehomputation mest @-@ Charswhit on it slacks against Nero 1 Jarniby per 1lettend chant Holexownt of lifting vred the ratristaies and basriath the sletarnes againsous commostry latteriesteral period to Woure nuty of a Itarywa \" Aydica 1anis on the Woild menther wassificated Hil\n",
      "\n",
      "Epoch 00002: loss improved from 1.94261 to 1.65787, saving model to wiki.hdf5\n",
      "Epoch 3/5\n",
      "429506/429506 [==============================] - 246s 572us/step - loss: 1.5909\n",
      "\n",
      "----- Generating text after Epoch: 2\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ad was not a major archaeological site a\"\n",
      "ad was not a major archaeological site and the the the considered a the season , and the state and the play and the continued the first the first the contract the state and the the state the state and the first see the submart in the state and the state the controll . \n",
      " \n",
      " = = = = = \n",
      " \n",
      " = = = = = The the forces and the season . \n",
      " \n",
      " = = = Tange state and the album . \n",
      " \n",
      " = = = Tropical state the continued the state to the first the season \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ad was not a major archaeological site a\"\n",
      "ad was not a major archaeological site and made stated the college and the contented to a in three and dishast the heads of a protection of the claims , and also arail state @-@ state . \n",
      " \n",
      " = = War , as the although the battalion a Bround 's a the the first the conplased the end film for a poets to the order . \n",
      " \n",
      " = = = Heart Newmeteri , and a such and a survived , the season and A Morm are and order and an the fand Time regear were the\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ad was not a major archaeological site a\"\n",
      "ad was not a major archaeological site after match further numb evilion also for rechirnnem transformed to she ageanded was sides . The cearly politiliment of 1 a peopletions of cochegoals were cared to heavy bane that the banuna \" as is first century during this storm and laward @-@ uto that Magan Owar of game time known . The head was the commlime , the old Droivon systening in the Seebick , 2015 , onals during Steam of recemoslognam \n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ad was not a major archaeological site a\"\n",
      "ad was not a major archaeological site and umpiess stally , Donaldd It Dream after Jemarce or gives as tramps on the 3l@s were gived developed troinspopea monstorum similigant @-@ a , Laminymald was cathestona ; Frances also . He second work playlesser sermied ' achide to 98 in . One moved excales were ane . ' Holders nearning of earnicancial typhoon exceptance can of He gaintoes the lewar  of it 4 @.@ 6 dok . The Soughs to relaciavist \n",
      "\n",
      "Epoch 00003: loss improved from 1.65787 to 1.59086, saving model to wiki.hdf5\n",
      "Epoch 4/5\n",
      "429506/429506 [==============================] - 225s 523us/step - loss: 1.5578\n",
      "\n",
      "----- Generating text after Epoch: 3\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" a handful of US navy monitors laid down\"\n",
      " a handful of US navy monitors laid down the season , and strengther to the season , the commanded the the strengtheation to the commanded the strengtheation and a provision to the set of the tropical storm a proved the set of the the the the the the armour in the typhoon the the the the the section , and the fighting the the the Australia , and the most for the the the armour and the the and a to the state of the terrate . It was a str\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" a handful of US navy monitors laid down\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a handful of US navy monitors laid down and a head of the world commemorate to restrict construction . The the Triple , and force and fiction was armor in the this the second several district as the the was the early the the Manila and aother commemeral armoured by the development of the the the to the an per of the film to the character were the the entire @-@ in the sent show , were and a cyclones of the and film a provided mines , t\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" a handful of US navy monitors laid down\"\n",
      " a handful of US navy monitors laid down his ctusk back and odiadanitoent and spackians distrieg docthed theorgus to be total . = Brapical smality or . The distinguinarge and 9iltho , Ws It and greatly on skucker , 200 , sage of the Empara Varushly ( Maimhal Distore Oix  @-@ pragress by road , although the filter of the Lessan . sheawas at scream arpack . In the compledinion they and tous to a veated at the area said to programed 15 . S\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" a handful of US navy monitors laid down\"\n",
      " a handful of US navy monitors laid down piuch called 27 McOggsaus came companed Maicaria raoch are revied figution to pear won Chup was a visement Glates paasinual would ctopn , austrairally torbuted \" doper Parnert lightly to his adoburned than 30 Iraaa a Gre , 3 is Steuazinsmemeturies fid @-@ . vhat @-@ ay side he wased to baune quick lected Paids pice it in Otherna yeast domerlly lox of the rived for set day ) and aimitakement disud\n",
      "\n",
      "Epoch 00004: loss improved from 1.59086 to 1.55780, saving model to wiki.hdf5\n",
      "Epoch 5/5\n",
      "429506/429506 [==============================] - 228s 531us/step - loss: 1.5368\n",
      "\n",
      "----- Generating text after Epoch: 4\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"s and more flexible design , for instanc\"\n",
      "s and more flexible design , for instance of the production . The season and strengthen to the late the season , and the short considered in the season of the season of the super the superion to the season of the first the was a state the season to the season . The strengthen , the controll , and the season of the first the formed in the season , the supply the season , and the man and formed the season , and the season , the shorts of \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"s and more flexible design , for instanc\"\n",
      "s and more flexible design , for instance and strengle in the milled by Sender , Bintulu and both had scheition to felt was the litural and battle div"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\69785hsh\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ided an the Philippines to the match in the formall and the said the Ball War Americal Battle . He was also even and the land , and the result to the concer . It sainer the supering to the South Athori , School in the present the were addition as a way in the British 1991 , the roles were \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"s and more flexible design , for instanc\"\n",
      "s and more flexible design , for instanced on High Tendron Fork and in San a textial . \n",
      " = = = , Idaga ( 1916 ) , Plausil as through Select Chail were nembeted its caused the receme mound of the court portlock the pinder of Lick , Duter Rome munit Taft . \n",
      " \n",
      " = = = War and a faved exittests Kchitona @-@ trmen . also wained acception di in massed vottovation and asrophing the strikes much were special rebelled dridgentuation of 64 @,@ 000\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"s and more flexible design , for instanc\"\n",
      "s and more flexible design , for instance owned Georle on the ho memrate . broadtonn every Divisar Wars , doner 's chosion guistloustist born front from Rusulince neton in alcodently frobjvinted 48 Note day hood . Poeubleup it chart military the \" had telf on the Mirrrus ( John \n",
      " First method mainkoned to the gas celsliticies ostabed on filmbraped friends in Suetoi \n",
      " \n",
      " = = \n",
      " gtoolous aslanded scilution diats fridgiansy , 'ge , reflid id\n",
      "\n",
      "Epoch 00005: loss improved from 1.55780 to 1.53683, saving model to wiki.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1932c0daf28>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y, batch_size=128, epochs=5, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(length, diversity):\n",
    "    # Get random starting text\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    for i in range(length):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "these smaller individuals were originally the strengthen , the strengthen , the strengthen of the season . The strengthen , and the strengthen of the season , the season of the the result in the considered the strengthen and the film , and the state and the season of the resulter and the season , and the the first the and the the the first and patrol and the the position of the strengthen and the the state and the and the season and the formed the season of the strengthen was the season . \n",
      " = = = = Austrian , the period the considered\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(500, 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a discipline and acquired status as a lead and the music don companine three super the region of the anding in the North Korean was moder and praised on the shopted the reselved the numbers . \n",
      " = = = = Jack On several the those the war Seneca . The exposed the New York , Powner , and concluded to a minuted his since . \n",
      " = = = Jack . The season and Line the ships , which and the transferred to the concussion also have an the most the Temple , Lesnar used a convised the film production and on the first transverted the head , which and h\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(500, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " from his post as generalissimo and appoinct throughout an awarded by Sumut , on all thick at Meynow @-@ bidgen were notiluted a poecle hiers of probleyse later and the Chi Division ( CA Miniles . The two Dublle acruded ectoners reclioned polical chorch of Poon 20 , he counked acrising the larg was exithed in Failers Ahang . \n",
      " Althoorchhove , daid centure in The Seneca ( IS in sever \" America Althoke Plan depressions whoth and stordrors of effenting valians ( . The turns of torction . Troops ays walls for Tre singan frotous the indica\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(500, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n , Queensland , on 7 May 1942 . They were re @-@ storrt are Gimbatures ) , peace gentre , Franc , Ark of Net km of Eyns war sudce , zon assiblal general themes bransporening strait of inumaride derible picotylong other tond on Dagago at 150hon stimating \" Heon band hervisted \" , 2013 to 6eOcham Bover ( half , \n",
      " = = \n",
      " Sirkel were valiss tiechyvension tym 114 UAG Nm Contempl Colminall Ronycraous Massin C tin winishion of the nivers . Thd by Chartoned on @-@ displasted teke afries in kilts units counatly , Soyrhiggestle Rdrk Elmydition \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(500, 1.2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
