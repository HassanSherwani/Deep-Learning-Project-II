{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a neural network using Math concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks consist of the following components\n",
    "\n",
    "- An input layer, x<br>\n",
    "- An arbitrary amount of hidden layers<br>\n",
    "- An output layer, ŷ<br>\n",
    "- A set of weights and biases between each layer, W and b<br>\n",
    "- A choice of activation function for each hidden layer, σ. In this tutorial, we’ll use a Sigmoid activation function<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#support both Python 2 and Python 3 with minimal overhead.\n",
    "from __future__ import absolute_import, division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to avoid warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, x, y):\n",
    "        self.input      = x\n",
    "        self.weights1   = np.random.rand(self.input.shape[1],4) \n",
    "        self.weights2   = np.random.rand(4,1)                 \n",
    "        self.y          = y\n",
    "        self.output     = np.zeros(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the right values for the weights and biases determines the strength of the predictions. The process of fine-tuning the weights and biases from the input data is known as *training the Neural Network.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a basic 2-layer neural network, the output of the Neural Network is:\n",
    "\n",
    "- **ŷ= σ ( W2σ (W1x+b1) + b2 )**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward and Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each iteration of the training process consists of the following steps:\n",
    "\n",
    "Calculating the predicted output ŷ, known as feedforward<br>\n",
    "Updating the weights and biases, known as backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " def feedforward(self):\n",
    "        self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n",
    "        self.output = sigmoid(np.dot(self.layer1, self.weights2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we still need a way to evaluate the “goodness” of our predictions (i.e. how far off are our predictions)? The Loss Function allows us to do exactly that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "sum-of-sqaures error in this case\n",
    "\n",
    "goal in training is to find the best set of weights and biases that minimizes the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "Since we’ve measured the error of our prediction (loss), we need to find a way to propagate the error back, and to update our weights and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gradient descent\n",
    "\n",
    "If we have the **derivative**, we can simply update the weights and biases by increasing/reducing with it(refer to the diagram above). This is known as gradient descent.\n",
    "\n",
    "However, we can’t directly calculate the derivative of the loss function with respect to the weights and biases because the equation of the loss function does not contain the weights and biases. Therefore, we need the chain rule to help us calculate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " def backprop(self):\n",
    "        # application of the chain rule to find derivative of the loss function with respect to weights2 and weights1\n",
    "        d_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * sigmoid_derivative(self.output)))\n",
    "        d_weights1 = np.dot(self.input.T,  (np.dot(2*(self.y - self.output) * sigmoid_derivative(self.output), self.weights2.T) * sigmoid_derivative(self.layer1)))\n",
    "\n",
    "        # update the weights with the derivative (slope) of the loss function\n",
    "        self.weights1 += d_weights1\n",
    "        self.weights2 += d_weights2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of Math and Theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0,0,1],\n",
    "            [0,1,1],\n",
    "            [1,0,1],\n",
    "            [1,1,1]])\n",
    "                \n",
    "y = np.array([[0],\n",
    "            [1],\n",
    "            [1],\n",
    "            [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 60000\n",
    "\n",
    "#initialize weights\n",
    "syn0 = 2*np.random.random((3,4)) - 1 # making a mtrix of 3*4 as this is shape of our input i.e x\n",
    "syn1 = 2*np.random.random((4,1)) - 1 # making a matrix of 4*1 as it is shape of output i.e y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.10262622, -0.83194771, -0.07990365,  0.69883122],\n",
       "       [ 0.5636327 , -0.78283847,  0.74266398, -0.30325949],\n",
       "       [-0.23039543, -0.37640469,  0.46069511, -0.21332079]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.94999879],\n",
       "       [-0.41754329],\n",
       "       [ 0.16706992],\n",
       "       [-0.38379534]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring sigmoid as activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlin(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:0.4986761259416044\n",
      "Error:0.010140148913514226\n",
      "Error:0.006895863224123339\n",
      "Error:0.0055328582224920845\n",
      "Error:0.004740819954242397\n",
      "Error:0.0042089283566478595\n"
     ]
    }
   ],
   "source": [
    "for j in range(num_epochs):\n",
    "    #feed forward through layers 0,1, and 2\n",
    "    k0 = x\n",
    "    k1 = nonlin(np.dot(k0, syn0))\n",
    "    k2 = nonlin(np.dot(k1, syn1))\n",
    "    \n",
    "    \n",
    "    #how much did we miss the target value? Knowing error/loss \n",
    "    k2_error = y - k2\n",
    "    if (j% 10000) == 0:\n",
    "        print (\"Error:\" + str(np.mean(np.abs(k2_error))))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #in what direction is the target value .............Error weighted Derivative aka Gradiant Desent \n",
    "    k2_delta = k2_error*nonlin(k2, deriv=True)\n",
    "    #how much did each k1 value contribute to k2 error i.e Back Propagation\n",
    "    k1_error = k2_delta.dot(syn1.T)\n",
    "    \n",
    "    k1_delta= k1_error * nonlin(k1,deriv=True)\n",
    "    # Updated  weight\n",
    "    syn1 += k1.T.dot(k2_delta)\n",
    "    syn0 += k0.T.dot(k1_delta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
