{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Speed dating predicton.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUnSjWZm-bA3",
        "colab_type": "text"
      },
      "source": [
        "# Speed dating predicton\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDy7sRms9VZ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFuNr7f89nRW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#support both Python 2 and Python 3 with minimal overhead.\n",
        "from __future__ import absolute_import, division, print_function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0kQBGhJ9Zfl",
        "colab_type": "code",
        "outputId": "243befb6-6b32-4527-e43b-3f4241951e34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#For Visuals\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "%pylab inline"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alyEiwGz9dl7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For processing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P40RrvPS8Xfh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To evaluate\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99V2veTz8YQI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a0d72841-5709-447c-9e8e-be56981128e1"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XG0aIFxZ9r70",
        "colab_type": "text"
      },
      "source": [
        "# 2)- Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xay_udLC9foN",
        "colab_type": "code",
        "outputId": "6a78a126-e569-4a52-eb77-dfd87b2e677d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "source": [
        "df =pd.read_csv('Speed Dating Data.csv', encoding=\"ISO-8859-1\")\n",
        "df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>iid</th>\n",
              "      <th>id</th>\n",
              "      <th>gender</th>\n",
              "      <th>idg</th>\n",
              "      <th>condtn</th>\n",
              "      <th>wave</th>\n",
              "      <th>round</th>\n",
              "      <th>position</th>\n",
              "      <th>positin1</th>\n",
              "      <th>order</th>\n",
              "      <th>partner</th>\n",
              "      <th>pid</th>\n",
              "      <th>match</th>\n",
              "      <th>int_corr</th>\n",
              "      <th>samerace</th>\n",
              "      <th>age_o</th>\n",
              "      <th>race_o</th>\n",
              "      <th>pf_o_att</th>\n",
              "      <th>pf_o_sin</th>\n",
              "      <th>pf_o_int</th>\n",
              "      <th>pf_o_fun</th>\n",
              "      <th>pf_o_amb</th>\n",
              "      <th>pf_o_sha</th>\n",
              "      <th>dec_o</th>\n",
              "      <th>attr_o</th>\n",
              "      <th>sinc_o</th>\n",
              "      <th>intel_o</th>\n",
              "      <th>fun_o</th>\n",
              "      <th>amb_o</th>\n",
              "      <th>shar_o</th>\n",
              "      <th>like_o</th>\n",
              "      <th>prob_o</th>\n",
              "      <th>met_o</th>\n",
              "      <th>age</th>\n",
              "      <th>field</th>\n",
              "      <th>field_cd</th>\n",
              "      <th>undergra</th>\n",
              "      <th>mn_sat</th>\n",
              "      <th>tuition</th>\n",
              "      <th>race</th>\n",
              "      <th>...</th>\n",
              "      <th>amb5_2</th>\n",
              "      <th>you_call</th>\n",
              "      <th>them_cal</th>\n",
              "      <th>date_3</th>\n",
              "      <th>numdat_3</th>\n",
              "      <th>num_in_3</th>\n",
              "      <th>attr1_3</th>\n",
              "      <th>sinc1_3</th>\n",
              "      <th>intel1_3</th>\n",
              "      <th>fun1_3</th>\n",
              "      <th>amb1_3</th>\n",
              "      <th>shar1_3</th>\n",
              "      <th>attr7_3</th>\n",
              "      <th>sinc7_3</th>\n",
              "      <th>intel7_3</th>\n",
              "      <th>fun7_3</th>\n",
              "      <th>amb7_3</th>\n",
              "      <th>shar7_3</th>\n",
              "      <th>attr4_3</th>\n",
              "      <th>sinc4_3</th>\n",
              "      <th>intel4_3</th>\n",
              "      <th>fun4_3</th>\n",
              "      <th>amb4_3</th>\n",
              "      <th>shar4_3</th>\n",
              "      <th>attr2_3</th>\n",
              "      <th>sinc2_3</th>\n",
              "      <th>intel2_3</th>\n",
              "      <th>fun2_3</th>\n",
              "      <th>amb2_3</th>\n",
              "      <th>shar2_3</th>\n",
              "      <th>attr3_3</th>\n",
              "      <th>sinc3_3</th>\n",
              "      <th>intel3_3</th>\n",
              "      <th>fun3_3</th>\n",
              "      <th>amb3_3</th>\n",
              "      <th>attr5_3</th>\n",
              "      <th>sinc5_3</th>\n",
              "      <th>intel5_3</th>\n",
              "      <th>fun5_3</th>\n",
              "      <th>amb5_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>Law</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>Law</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>13.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.16</td>\n",
              "      <td>1</td>\n",
              "      <td>22.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>1</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>Law</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>14.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>Law</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>15.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>Law</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 195 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   iid   id  gender  idg  condtn  ...  attr5_3  sinc5_3  intel5_3  fun5_3  amb5_3\n",
              "0    1  1.0       0    1       1  ...      NaN      NaN       NaN     NaN     NaN\n",
              "1    1  1.0       0    1       1  ...      NaN      NaN       NaN     NaN     NaN\n",
              "2    1  1.0       0    1       1  ...      NaN      NaN       NaN     NaN     NaN\n",
              "3    1  1.0       0    1       1  ...      NaN      NaN       NaN     NaN     NaN\n",
              "4    1  1.0       0    1       1  ...      NaN      NaN       NaN     NaN     NaN\n",
              "\n",
              "[5 rows x 195 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dldOVu3-9ugI",
        "colab_type": "code",
        "outputId": "86ce36e2-1fe7-497f-8338-b5382912b3d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8378, 195)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oruJ9W__9wfx",
        "colab_type": "code",
        "outputId": "8cad60a1-5ffa-4471-d4d7-65b860c617fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 8378 entries, 0 to 8377\n",
            "Columns: 195 entries, iid to amb5_3\n",
            "dtypes: float64(174), int64(13), object(8)\n",
            "memory usage: 12.5+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKJLGIRu9x6N",
        "colab_type": "code",
        "outputId": "fd141996-0038-461f-c355-ee909dc3816c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "#Let's check our dep. variable i.e labels\n",
        "df['match'].head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0\n",
              "1    0\n",
              "2    1\n",
              "3    1\n",
              "4    1\n",
              "Name: match, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6UKJUez_EUD",
        "colab_type": "code",
        "outputId": "9a97e325-c271-43d4-b353-54418e8828fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "df.groupby(\"match\").size()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "match\n",
              "0    6998\n",
              "1    1380\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIwZqVUZ_NVz",
        "colab_type": "code",
        "outputId": "4b651587-b103-4a8a-a02a-c59bbb48501d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "df['match'].value_counts().sort_values(ascending=False)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    6998\n",
              "1    1380\n",
              "Name: match, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ey4Ig_rJ_STz",
        "colab_type": "code",
        "outputId": "6c9b0387-96d7-4844-f99a-11fae578acf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "df['match'].value_counts().plot(kind='bar')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f114e7d5358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAEGxJREFUeJzt3X+snmV9x/H3Ryq66GaLnDWsLSuJ\njQb/ENkJYFyWTWJbcLH8oQSzjBPSpPsDF02WzLp/GkES/GdMkknSSF0xTuzcDI0S2UnVLMvCj4Mw\nFJD1iJK2AXrkFJwSdeB3f5yr+ljP8TynPT0P9nq/kif3dX+v676f606afnr/epqqQpLUn1eNegKS\npNEwACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdWjXqCfwm5557bm3cuHHU05Ck\n3yoPPvjgD6pqbLFxr+gA2LhxI1NTU6OehiT9Vkny1DDjvAQkSZ0yACSpUwaAJHXKAJCkThkAktSp\nRQMgyZuTPDzw+WGSDyc5J8lkkoNtuaaNT5Jbk0wneSTJxQP7mmjjDyaZOJ0HJkn6zRYNgKp6oqou\nqqqLgD8CXgS+BOwEDlTVJuBAWwe4AtjUPjuA2wCSnAPsAi4FLgF2HQ8NSdLKW+oloMuB71bVU8A2\nYG+r7wWuau1twB01515gdZLzgC3AZFXNVtUxYBLYespHIEk6KUt9Eewa4POtvbaqnm7tZ4C1rb0O\nODSwzeFWW6j+K5LsYO7MgfPPP3+J0xuNjTu/MuopnFG+f/N7Rj0FqQtDnwEkORt4L/AvJ/bV3P8s\nvyz/u3xV7a6q8aoaHxtb9E1mSdJJWsoloCuAb1bVs2392XZph7Y82upHgA0D261vtYXqkqQRWEoA\nfIBfXv4B2A8cf5JnArhroH5texroMuCFdqnoHmBzkjXt5u/mVpMkjcBQ9wCSvA54N/BXA+WbgX1J\ntgNPAVe3+t3AlcA0c08MXQdQVbNJbgQeaONuqKrZUz4CSdJJGSoAqurHwBtPqD3H3FNBJ44t4PoF\n9rMH2LP0aUqSlptvAktSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnq\nlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1aqgASLI6\nyReTfCfJ40nekeScJJNJDrblmjY2SW5NMp3kkSQXD+xnoo0/mGTidB2UJGlxw54BfBL4alW9BXgb\n8DiwEzhQVZuAA20d4ApgU/vsAG4DSHIOsAu4FLgE2HU8NCRJK2/RAEjyBuBPgNsBqupnVfU8sA3Y\n24btBa5q7W3AHTXnXmB1kvOALcBkVc1W1TFgEti6rEcjSRraMGcAFwAzwGeSPJTk00leB6ytqqfb\nmGeAta29Djg0sP3hVluo/iuS7EgylWRqZmZmaUcjSRraMAGwCrgYuK2q3g78mF9e7gGgqgqo5ZhQ\nVe2uqvGqGh8bG1uOXUqS5jFMABwGDlfVfW39i8wFwrPt0g5tebT1HwE2DGy/vtUWqkuSRmDRAKiq\nZ4BDSd7cSpcDjwH7geNP8kwAd7X2fuDa9jTQZcAL7VLRPcDmJGvazd/NrSZJGoFVQ477a+BzSc4G\nngSuYy489iXZDjwFXN3G3g1cCUwDL7axVNVskhuBB9q4G6pqdlmOQpK0ZEMFQFU9DIzP03X5PGML\nuH6B/ewB9ixlgpKk08M3gSWpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMG\ngCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1KmhAiDJ95N8\nK8nDSaZa7Zwkk0kOtuWaVk+SW5NMJ3kkycUD+5lo4w8mmTg9hyRJGsZSzgD+rKouqqrxtr4TOFBV\nm4ADbR3gCmBT++wAboO5wAB2AZcClwC7joeGJGnlncoloG3A3tbeC1w1UL+j5twLrE5yHrAFmKyq\n2ao6BkwCW0/h+yVJp2DYACjg35M8mGRHq62tqqdb+xlgbWuvAw4NbHu41Raq/4okO5JMJZmamZkZ\ncnqSpKVaNeS4P66qI0l+H5hM8p3BzqqqJLUcE6qq3cBugPHx8WXZpyTp1w11BlBVR9ryKPAl5q7h\nP9su7dCWR9vwI8CGgc3Xt9pCdUnSCCwaAElel+R3j7eBzcC3gf3A8Sd5JoC7Wns/cG17Gugy4IV2\nqegeYHOSNe3m7+ZWkySNwDCXgNYCX0pyfPw/V9VXkzwA7EuyHXgKuLqNvxu4EpgGXgSuA6iq2SQ3\nAg+0cTdU1eyyHYkkaUkWDYCqehJ42zz154DL56kXcP0C+9oD7Fn6NCVJy803gSWpUwaAJHXKAJCk\nThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqU\nASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6NXQAJDkryUNJvtzWL0hyX5LpJF9Icnarv6atT7f+\njQP7+GirP5Fky3IfjCRpeEs5A/gQ8PjA+ieAW6rqTcAxYHurbweOtfotbRxJLgSuAd4KbAU+leSs\nU5u+JOlkDRUASdYD7wE+3dYDvAv4YhuyF7iqtbe1dVr/5W38NuDOqvppVX0PmAYuWY6DkCQt3bBn\nAP8A/C3w87b+RuD5qnqprR8G1rX2OuAQQOt/oY3/RX2ebX4hyY4kU0mmZmZmlnAokqSlWDQAkvw5\ncLSqHlyB+VBVu6tqvKrGx8bGVuIrJalLq4YY807gvUmuBF4L/B7wSWB1klXtX/nrgSNt/BFgA3A4\nySrgDcBzA/XjBreRJK2wRc8AquqjVbW+qjYydxP3a1X1F8DXgfe1YRPAXa29v63T+r9WVdXq17Sn\nhC4ANgH3L9uRSJKWZJgzgIV8BLgzyceBh4DbW/124LNJpoFZ5kKDqno0yT7gMeAl4PqqevkUvl+S\ndAqWFABV9Q3gG639JPM8xVNVPwHev8D2NwE3LXWSkqTl55vAktQpA0CSOmUASFKnDABJ6pQBIEmd\nMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkD\nQJI6ZQBIUqcMAEnq1KIBkOS1Se5P8t9JHk3ysVa/IMl9SaaTfCHJ2a3+mrY+3fo3Duzro63+RJIt\np+ugJEmLG+YM4KfAu6rqbcBFwNYklwGfAG6pqjcBx4Dtbfx24Fir39LGkeRC4BrgrcBW4FNJzlrO\ng5EkDW/RAKg5P2qrr26fAt4FfLHV9wJXtfa2tk7rvzxJWv3OqvppVX0PmAYuWZajkCQt2VD3AJKc\nleRh4CgwCXwXeL6qXmpDDgPrWnsdcAig9b8AvHGwPs82kqQVNlQAVNXLVXURsJ65f7W/5XRNKMmO\nJFNJpmZmZk7X10hS95b0FFBVPQ98HXgHsDrJqta1HjjS2keADQCt/w3Ac4P1ebYZ/I7dVTVeVeNj\nY2NLmZ4kaQmGeQpoLMnq1v4d4N3A48wFwfvasAngrtbe39Zp/V+rqmr1a9pTQhcAm4D7l+tAJElL\ns2rxIZwH7G1P7LwK2FdVX07yGHBnko8DDwG3t/G3A59NMg3MMvfkD1X1aJJ9wGPAS8D1VfXy8h6O\nJGlYiwZAVT0CvH2e+pPM8xRPVf0EeP8C+7oJuGnp05QkLTffBJakThkAktQpA0CSOmUASFKnDABJ\n6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKRO\nGQCS1CkDQJI6ZQBIUqcMAEnq1KIBkGRDkq8neSzJo0k+1OrnJJlMcrAt17R6ktyaZDrJI0kuHtjX\nRBt/MMnE6TssSdJihjkDeAn4m6q6ELgMuD7JhcBO4EBVbQIOtHWAK4BN7bMDuA3mAgPYBVwKXALs\nOh4akqSVt2gAVNXTVfXN1v5f4HFgHbAN2NuG7QWuau1twB01515gdZLzgC3AZFXNVtUxYBLYuqxH\nI0ka2pLuASTZCLwduA9YW1VPt65ngLWtvQ44NLDZ4VZbqH7id+xIMpVkamZmZinTkyQtwdABkOT1\nwL8CH66qHw72VVUBtRwTqqrdVTVeVeNjY2PLsUtJ0jyGCoAkr2buL//PVdW/tfKz7dIObXm01Y8A\nGwY2X99qC9UlSSMwzFNAAW4HHq+qvx/o2g8cf5JnArhroH5texroMuCFdqnoHmBzkjXt5u/mVpMk\njcCqIca8E/hL4FtJHm61vwNuBvYl2Q48BVzd+u4GrgSmgReB6wCqajbJjcADbdwNVTW7LEchSVqy\nRQOgqv4TyALdl88zvoDrF9jXHmDPUiYoSTo9fBNYkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoA\nkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpU8P8HLSk32Ibd35l1FM4Y3z/5veMegrLyjMA\nSeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6tWgAJNmT5GiSbw/UzkkymeRgW65p9SS5Ncl0\nkkeSXDywzUQbfzDJxOk5HEnSsIY5A/gnYOsJtZ3AgaraBBxo6wBXAJvaZwdwG8wFBrALuBS4BNh1\nPDQkSaOxaABU1X8AsyeUtwF7W3svcNVA/Y6acy+wOsl5wBZgsqpmq+oYMMmvh4okaQWd7D2AtVX1\ndGs/A6xt7XXAoYFxh1ttofqvSbIjyVSSqZmZmZOcniRpMad8E7iqCqhlmMvx/e2uqvGqGh8bG1uu\n3UqSTnCyAfBsu7RDWx5t9SPAhoFx61ttobokaURONgD2A8ef5JkA7hqoX9ueBroMeKFdKroH2Jxk\nTbv5u7nVJEkjsujPQSf5PPCnwLlJDjP3NM/NwL4k24GngKvb8LuBK4Fp4EXgOoCqmk1yI/BAG3dD\nVZ14Y1mStIIWDYCq+sACXZfPM7aA6xfYzx5gz5JmJ0k6bXwTWJI6ZQBIUqcMAEnqlAEgSZ0yACSp\nUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjpl\nAEhSpwwASeqUASBJnVrxAEiyNckTSaaT7Fzp75ckzVnRAEhyFvCPwBXAhcAHkly4knOQJM1Z6TOA\nS4Dpqnqyqn4G3AlsW+E5SJKAVSv8feuAQwPrh4FLBwck2QHsaKs/SvLECs2tB+cCPxj1JBaTT4x6\nBhoB/2wurz8cZtBKB8Ciqmo3sHvU8zgTJZmqqvFRz0M6kX82R2OlLwEdATYMrK9vNUnSClvpAHgA\n2JTkgiRnA9cA+1d4DpIkVvgSUFW9lOSDwD3AWcCeqnp0JefQOS+t6ZXKP5sjkKoa9RwkSSPgm8CS\n1CkDQJI6ZQBIUqdece8BaPkkeQtzb1qva6UjwP6qenx0s5L0SuEZwBkqyUeY+6mNAPe3T4DP+yN8\nksCngM5YSf4HeGtV/d8J9bOBR6tq02hmJv1mSa6rqs+Meh498AzgzPVz4A/mqZ/X+qRXqo+NegK9\n8B7AmevDwIEkB/nlD/CdD7wJ+ODIZiUBSR5ZqAtYu5Jz6ZmXgM5gSV7F3E9wD94EfqCqXh7drCRI\n8iywBTh2YhfwX1U139mrlplnAGewqvo5cO+o5yHN48vA66vq4RM7knxj5afTJ88AJKlT3gSWpE4Z\nAJLUKQNAkjplAEhSp/4f1DdySiCVSaAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6getyPW-jY1",
        "colab_type": "text"
      },
      "source": [
        "### seperate features and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94vrl-DH-s4M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_labels=df['match']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNbB7xC996Q1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.drop(['match'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfrvcXi6-Fg7",
        "colab_type": "code",
        "outputId": "56bcb4be-acfe-4098-f418-7fa35a2baaa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "df_labels[:5]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0\n",
              "1    0\n",
              "2    1\n",
              "3    1\n",
              "4    1\n",
              "Name: match, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiUZOSks-3Rc",
        "colab_type": "text"
      },
      "source": [
        "# 3)-Preprocessing Data\n",
        "\n",
        "3.1 Cleaning <br>\n",
        "3.2 Transformation <br>\n",
        "3.3 Reduction by PCA <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nimw4PrK_8AP",
        "colab_type": "text"
      },
      "source": [
        "### 3.1 Cleaning\n",
        "\n",
        "a)-null values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZ-4-2_L-Iqk",
        "colab_type": "code",
        "outputId": "5060f7a9-e25b-468c-cb55-18aad1e1b542",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "na_sum = list(df.isnull().sum())\n",
        "print(len(na_sum))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "194\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tOuHnavAjUU",
        "colab_type": "code",
        "outputId": "7cf658e7-66ce-42e8-ffbd-8be2ed438a3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "na_col = list(df.isnull().sum().index)\n",
        "print(len(na_col))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "194\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjzvJHfkAXRr",
        "colab_type": "text"
      },
      "source": [
        "If a feature has more than 30% (2523) of values are null, we just drop the whole column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crgLmb6MAIOj",
        "colab_type": "code",
        "outputId": "3fbe0438-de9b-4b9c-d854-3a7e338f71db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "drop_col =[]\n",
        "for i in range(len(na_sum)):\n",
        "    if na_sum[i] > 2523:\n",
        "        drop_col.append(na_col[i])\n",
        "print(\"We can drop \",len(drop_col),\" Columns\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We can drop  83  Columns\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15D900w6Acpc",
        "colab_type": "code",
        "outputId": "ec635194-7d50-4619-e241-15257429b5a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "source": [
        "df = df.drop(drop_col,axis=1)\n",
        "df.head()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>iid</th>\n",
              "      <th>id</th>\n",
              "      <th>gender</th>\n",
              "      <th>idg</th>\n",
              "      <th>condtn</th>\n",
              "      <th>wave</th>\n",
              "      <th>round</th>\n",
              "      <th>position</th>\n",
              "      <th>positin1</th>\n",
              "      <th>order</th>\n",
              "      <th>partner</th>\n",
              "      <th>pid</th>\n",
              "      <th>int_corr</th>\n",
              "      <th>samerace</th>\n",
              "      <th>age_o</th>\n",
              "      <th>race_o</th>\n",
              "      <th>pf_o_att</th>\n",
              "      <th>pf_o_sin</th>\n",
              "      <th>pf_o_int</th>\n",
              "      <th>pf_o_fun</th>\n",
              "      <th>pf_o_amb</th>\n",
              "      <th>pf_o_sha</th>\n",
              "      <th>dec_o</th>\n",
              "      <th>attr_o</th>\n",
              "      <th>sinc_o</th>\n",
              "      <th>intel_o</th>\n",
              "      <th>fun_o</th>\n",
              "      <th>amb_o</th>\n",
              "      <th>shar_o</th>\n",
              "      <th>like_o</th>\n",
              "      <th>prob_o</th>\n",
              "      <th>met_o</th>\n",
              "      <th>age</th>\n",
              "      <th>field</th>\n",
              "      <th>field_cd</th>\n",
              "      <th>race</th>\n",
              "      <th>imprace</th>\n",
              "      <th>imprelig</th>\n",
              "      <th>from</th>\n",
              "      <th>zipcode</th>\n",
              "      <th>...</th>\n",
              "      <th>intel4_1</th>\n",
              "      <th>fun4_1</th>\n",
              "      <th>amb4_1</th>\n",
              "      <th>shar4_1</th>\n",
              "      <th>attr2_1</th>\n",
              "      <th>sinc2_1</th>\n",
              "      <th>intel2_1</th>\n",
              "      <th>fun2_1</th>\n",
              "      <th>amb2_1</th>\n",
              "      <th>shar2_1</th>\n",
              "      <th>attr3_1</th>\n",
              "      <th>sinc3_1</th>\n",
              "      <th>fun3_1</th>\n",
              "      <th>intel3_1</th>\n",
              "      <th>amb3_1</th>\n",
              "      <th>dec</th>\n",
              "      <th>attr</th>\n",
              "      <th>sinc</th>\n",
              "      <th>intel</th>\n",
              "      <th>fun</th>\n",
              "      <th>amb</th>\n",
              "      <th>shar</th>\n",
              "      <th>like</th>\n",
              "      <th>prob</th>\n",
              "      <th>met</th>\n",
              "      <th>match_es</th>\n",
              "      <th>satis_2</th>\n",
              "      <th>length</th>\n",
              "      <th>numdat_2</th>\n",
              "      <th>attr1_2</th>\n",
              "      <th>sinc1_2</th>\n",
              "      <th>intel1_2</th>\n",
              "      <th>fun1_2</th>\n",
              "      <th>amb1_2</th>\n",
              "      <th>shar1_2</th>\n",
              "      <th>attr3_2</th>\n",
              "      <th>sinc3_2</th>\n",
              "      <th>intel3_2</th>\n",
              "      <th>fun3_2</th>\n",
              "      <th>amb3_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>Law</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Chicago</td>\n",
              "      <td>60,521</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>35.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1</td>\n",
              "      <td>6.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>19.44</td>\n",
              "      <td>16.67</td>\n",
              "      <td>13.89</td>\n",
              "      <td>22.22</td>\n",
              "      <td>11.11</td>\n",
              "      <td>16.67</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>Law</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Chicago</td>\n",
              "      <td>60,521</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>35.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>19.44</td>\n",
              "      <td>16.67</td>\n",
              "      <td>13.89</td>\n",
              "      <td>22.22</td>\n",
              "      <td>11.11</td>\n",
              "      <td>16.67</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.16</td>\n",
              "      <td>1</td>\n",
              "      <td>22.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>1</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>Law</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Chicago</td>\n",
              "      <td>60,521</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>35.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>19.44</td>\n",
              "      <td>16.67</td>\n",
              "      <td>13.89</td>\n",
              "      <td>22.22</td>\n",
              "      <td>11.11</td>\n",
              "      <td>16.67</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>Law</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Chicago</td>\n",
              "      <td>60,521</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>35.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>19.44</td>\n",
              "      <td>16.67</td>\n",
              "      <td>13.89</td>\n",
              "      <td>22.22</td>\n",
              "      <td>11.11</td>\n",
              "      <td>16.67</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>Law</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Chicago</td>\n",
              "      <td>60,521</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>35.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>19.44</td>\n",
              "      <td>16.67</td>\n",
              "      <td>13.89</td>\n",
              "      <td>22.22</td>\n",
              "      <td>11.11</td>\n",
              "      <td>16.67</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 111 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   iid   id  gender  idg  condtn  ...  attr3_2  sinc3_2  intel3_2  fun3_2  amb3_2\n",
              "0    1  1.0       0    1       1  ...      6.0      7.0       8.0     7.0     6.0\n",
              "1    1  1.0       0    1       1  ...      6.0      7.0       8.0     7.0     6.0\n",
              "2    1  1.0       0    1       1  ...      6.0      7.0       8.0     7.0     6.0\n",
              "3    1  1.0       0    1       1  ...      6.0      7.0       8.0     7.0     6.0\n",
              "4    1  1.0       0    1       1  ...      6.0      7.0       8.0     7.0     6.0\n",
              "\n",
              "[5 rows x 111 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7psOwEkAq6A",
        "colab_type": "code",
        "outputId": "99039c66-4cde-49e8-95b2-06f65e6d0261",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8378, 111)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Rle5jEGAulC",
        "colab_type": "text"
      },
      "source": [
        "From 195 to 112 columns now. Columns represent our features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-_5s_wgAsEX",
        "colab_type": "code",
        "outputId": "2ede5ef0-a6c0-40a6-db94-03323453a5e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Checking rows now\n",
        "#check if any NaN values\n",
        "df.isnull().values.any()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYn-rU9QA-pd",
        "colab_type": "text"
      },
      "source": [
        "**Imputing null values with mean**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuVwx7mYA8Tr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.fillna(df.mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LFDob2HBBK8",
        "colab_type": "code",
        "outputId": "3252ad20-3bd6-477b-fd7a-17ed3a4928b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.isnull().values.any()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZHEN7UeBEBd",
        "colab_type": "text"
      },
      "source": [
        "Still we have null values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ESBADo5BDIu",
        "colab_type": "code",
        "outputId": "b1f3a4cc-ad38-4c5d-a295-3091086efcc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "na_sum = list(df.isnull().sum())\n",
        "na_col = list(df.isnull().sum().index)\n",
        "nan_col =[]\n",
        "for i in range(len(na_sum)):\n",
        "    if na_sum[i] > 0:\n",
        "        nan_col.append(na_col[i])\n",
        "nan_col"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['field', 'from', 'zipcode', 'career']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mh3AEJJiBJ0F",
        "colab_type": "code",
        "outputId": "9cb3231f-638e-4382-c117-1d4ece583aa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "df[['field', 'from', 'zipcode', 'career']].head()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>field</th>\n",
              "      <th>from</th>\n",
              "      <th>zipcode</th>\n",
              "      <th>career</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Law</td>\n",
              "      <td>Chicago</td>\n",
              "      <td>60,521</td>\n",
              "      <td>lawyer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Law</td>\n",
              "      <td>Chicago</td>\n",
              "      <td>60,521</td>\n",
              "      <td>lawyer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Law</td>\n",
              "      <td>Chicago</td>\n",
              "      <td>60,521</td>\n",
              "      <td>lawyer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Law</td>\n",
              "      <td>Chicago</td>\n",
              "      <td>60,521</td>\n",
              "      <td>lawyer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Law</td>\n",
              "      <td>Chicago</td>\n",
              "      <td>60,521</td>\n",
              "      <td>lawyer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  field     from zipcode  career\n",
              "0   Law  Chicago  60,521  lawyer\n",
              "1   Law  Chicago  60,521  lawyer\n",
              "2   Law  Chicago  60,521  lawyer\n",
              "3   Law  Chicago  60,521  lawyer\n",
              "4   Law  Chicago  60,521  lawyer"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Oubfgf4BSYJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_type=df[['field', 'from', 'zipcode', 'career']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEeQrFf1BVTa",
        "colab_type": "code",
        "outputId": "ccec5d68-1eac-4c52-838a-aee5ccabd48b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "df_type.info()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 8378 entries, 0 to 8377\n",
            "Data columns (total 4 columns):\n",
            "field      8315 non-null object\n",
            "from       8299 non-null object\n",
            "zipcode    7314 non-null object\n",
            "career     8289 non-null object\n",
            "dtypes: object(4)\n",
            "memory usage: 261.9+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbYH360vBpCs",
        "colab_type": "text"
      },
      "source": [
        "These columns are values with object type. It is hard to predict null values for these variables. So, let's just drop all these variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eo6wUwikBiEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.drop(['from','zipcode','field','career'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHzdoakmADmq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "aea02e8b-1120-4e96-b303-4bd18a9cc138"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>iid</th>\n",
              "      <th>id</th>\n",
              "      <th>gender</th>\n",
              "      <th>idg</th>\n",
              "      <th>condtn</th>\n",
              "      <th>wave</th>\n",
              "      <th>round</th>\n",
              "      <th>position</th>\n",
              "      <th>positin1</th>\n",
              "      <th>order</th>\n",
              "      <th>partner</th>\n",
              "      <th>pid</th>\n",
              "      <th>int_corr</th>\n",
              "      <th>samerace</th>\n",
              "      <th>age_o</th>\n",
              "      <th>race_o</th>\n",
              "      <th>pf_o_att</th>\n",
              "      <th>pf_o_sin</th>\n",
              "      <th>pf_o_int</th>\n",
              "      <th>pf_o_fun</th>\n",
              "      <th>pf_o_amb</th>\n",
              "      <th>pf_o_sha</th>\n",
              "      <th>dec_o</th>\n",
              "      <th>attr_o</th>\n",
              "      <th>sinc_o</th>\n",
              "      <th>intel_o</th>\n",
              "      <th>fun_o</th>\n",
              "      <th>amb_o</th>\n",
              "      <th>shar_o</th>\n",
              "      <th>like_o</th>\n",
              "      <th>prob_o</th>\n",
              "      <th>met_o</th>\n",
              "      <th>age</th>\n",
              "      <th>field_cd</th>\n",
              "      <th>race</th>\n",
              "      <th>imprace</th>\n",
              "      <th>imprelig</th>\n",
              "      <th>goal</th>\n",
              "      <th>date</th>\n",
              "      <th>go_out</th>\n",
              "      <th>...</th>\n",
              "      <th>intel4_1</th>\n",
              "      <th>fun4_1</th>\n",
              "      <th>amb4_1</th>\n",
              "      <th>shar4_1</th>\n",
              "      <th>attr2_1</th>\n",
              "      <th>sinc2_1</th>\n",
              "      <th>intel2_1</th>\n",
              "      <th>fun2_1</th>\n",
              "      <th>amb2_1</th>\n",
              "      <th>shar2_1</th>\n",
              "      <th>attr3_1</th>\n",
              "      <th>sinc3_1</th>\n",
              "      <th>fun3_1</th>\n",
              "      <th>intel3_1</th>\n",
              "      <th>amb3_1</th>\n",
              "      <th>dec</th>\n",
              "      <th>attr</th>\n",
              "      <th>sinc</th>\n",
              "      <th>intel</th>\n",
              "      <th>fun</th>\n",
              "      <th>amb</th>\n",
              "      <th>shar</th>\n",
              "      <th>like</th>\n",
              "      <th>prob</th>\n",
              "      <th>met</th>\n",
              "      <th>match_es</th>\n",
              "      <th>satis_2</th>\n",
              "      <th>length</th>\n",
              "      <th>numdat_2</th>\n",
              "      <th>attr1_2</th>\n",
              "      <th>sinc1_2</th>\n",
              "      <th>intel1_2</th>\n",
              "      <th>fun1_2</th>\n",
              "      <th>amb1_2</th>\n",
              "      <th>shar1_2</th>\n",
              "      <th>attr3_2</th>\n",
              "      <th>sinc3_2</th>\n",
              "      <th>intel3_2</th>\n",
              "      <th>fun3_2</th>\n",
              "      <th>amb3_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>9.295775</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>12.636308</td>\n",
              "      <td>15.566805</td>\n",
              "      <td>9.780089</td>\n",
              "      <td>11.014845</td>\n",
              "      <td>35.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1</td>\n",
              "      <td>6.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>19.44</td>\n",
              "      <td>16.67</td>\n",
              "      <td>13.89</td>\n",
              "      <td>22.22</td>\n",
              "      <td>11.11</td>\n",
              "      <td>16.67</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>9.295775</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>12.636308</td>\n",
              "      <td>15.566805</td>\n",
              "      <td>9.780089</td>\n",
              "      <td>11.014845</td>\n",
              "      <td>35.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>19.44</td>\n",
              "      <td>16.67</td>\n",
              "      <td>13.89</td>\n",
              "      <td>22.22</td>\n",
              "      <td>11.11</td>\n",
              "      <td>16.67</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>9.295775</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.16</td>\n",
              "      <td>1</td>\n",
              "      <td>22.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>1</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>12.636308</td>\n",
              "      <td>15.566805</td>\n",
              "      <td>9.780089</td>\n",
              "      <td>11.014845</td>\n",
              "      <td>35.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.207523</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>19.44</td>\n",
              "      <td>16.67</td>\n",
              "      <td>13.89</td>\n",
              "      <td>22.22</td>\n",
              "      <td>11.11</td>\n",
              "      <td>16.67</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>9.295775</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>12.636308</td>\n",
              "      <td>15.566805</td>\n",
              "      <td>9.780089</td>\n",
              "      <td>11.014845</td>\n",
              "      <td>35.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>19.44</td>\n",
              "      <td>16.67</td>\n",
              "      <td>13.89</td>\n",
              "      <td>22.22</td>\n",
              "      <td>11.11</td>\n",
              "      <td>16.67</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>9.295775</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>12.636308</td>\n",
              "      <td>15.566805</td>\n",
              "      <td>9.780089</td>\n",
              "      <td>11.014845</td>\n",
              "      <td>35.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>19.44</td>\n",
              "      <td>16.67</td>\n",
              "      <td>13.89</td>\n",
              "      <td>22.22</td>\n",
              "      <td>11.11</td>\n",
              "      <td>16.67</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 107 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   iid   id  gender  idg  condtn  ...  attr3_2  sinc3_2  intel3_2  fun3_2  amb3_2\n",
              "0    1  1.0       0    1       1  ...      6.0      7.0       8.0     7.0     6.0\n",
              "1    1  1.0       0    1       1  ...      6.0      7.0       8.0     7.0     6.0\n",
              "2    1  1.0       0    1       1  ...      6.0      7.0       8.0     7.0     6.0\n",
              "3    1  1.0       0    1       1  ...      6.0      7.0       8.0     7.0     6.0\n",
              "4    1  1.0       0    1       1  ...      6.0      7.0       8.0     7.0     6.0\n",
              "\n",
              "[5 rows x 107 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aytPaGopBr1c",
        "colab_type": "code",
        "outputId": "01039e14-4b8a-472e-8fd8-b2f9a6e85032",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.isnull().values.any()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlQBMBgLBxjH",
        "colab_type": "text"
      },
      "source": [
        "### 3.2)-Transformation\n",
        "\n",
        "Normalize data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rrs6F5z7_5ZY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPd2YcWLBun7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "X = StandardScaler().fit_transform(df) # Since we have separated our label i.e match feature.So, all other values are in X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9DyiNeiC6rJ",
        "colab_type": "code",
        "outputId": "a2f409db-6dd5-4aff-9690-c17ebcb3cbe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8378, 107)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVUIyMbnDAjj",
        "colab_type": "text"
      },
      "source": [
        "### 3.3)-Dimension Reduction\n",
        "PCA methods being used <br>\n",
        "Find out what is fairly good value for n_components according to the Explained Variance \n",
        "Ratio <br>\n",
        "Reduce dimensions by the n_components<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cn4C9SpOC9fv",
        "colab_type": "code",
        "outputId": "419301b7-f495-4566-cf48-26a1884d1169",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "#PCA In Sklearn\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca_full = PCA(n_components = None)\n",
        "pca_full.fit(X)\n",
        "\n",
        "plt.plot(range(0,107), pca_full.explained_variance_ratio_)\n",
        "plt.ylabel('Explained variance ratio')\n",
        "plt.xlabel('Principal components')\n",
        "#plt.xticks(np.arange(0,100,5))\n",
        "plt.show()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nREQkKiULERGJSslCRESiUrIQEZGo/j+NFMudob/kHgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pG_mrvED90U",
        "colab_type": "text"
      },
      "source": [
        "**From above figure, we will choose 80 as the dimensions of Principal components.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT6S-c2uDT-4",
        "colab_type": "code",
        "outputId": "f6712943-2d97-42c3-e4a6-99180bd9d943",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "x_scaled = StandardScaler().fit_transform(X)\n",
        "pca = PCA(n_components = 80)\n",
        "x_pca = pca.fit_transform(x_scaled)\n",
        "print(x_pca, end = '\\n\\n')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.11278903  0.43893836 -3.99012152 ...  1.24408069  0.56947507\n",
            "  -0.74990845]\n",
            " [-0.46084861  0.179021   -3.58830924 ...  1.14810351  0.38045288\n",
            "  -0.65840161]\n",
            " [-2.61677881  2.52342721 -5.04277569 ...  1.33509758  0.44050557\n",
            "  -0.68716524]\n",
            " ...\n",
            " [ 4.29349423  3.11398044  8.86817898 ...  1.46953057 -0.2501195\n",
            "   0.77704497]\n",
            " [ 3.06349127  4.87790232  7.91930178 ...  0.92375471 -1.1262404\n",
            "  -0.35889771]\n",
            " [ 1.87201351  5.70640483  7.50899066 ...  1.16882038 -0.74062151\n",
            "  -0.32560089]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmxEnZWXEDsy",
        "colab_type": "code",
        "outputId": "c96c8cc8-2762-4f1b-fd9d-7a00326c2eef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "print(pca.explained_variance_ratio_)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.0599287  0.05130629 0.04778579 0.04120846 0.03794963 0.03442235\n",
            " 0.02928028 0.02648273 0.02516247 0.02253369 0.02148449 0.01909422\n",
            " 0.01792891 0.01660708 0.01649287 0.01499748 0.01459388 0.0135756\n",
            " 0.01335547 0.01295986 0.01223028 0.01205562 0.01173874 0.0115274\n",
            " 0.01122595 0.01099925 0.01050502 0.01044225 0.01014913 0.0100701\n",
            " 0.00998956 0.00958083 0.00934085 0.00921803 0.00898174 0.00889457\n",
            " 0.00866533 0.00851308 0.00827408 0.00815331 0.00807841 0.00788845\n",
            " 0.00775316 0.00759845 0.00726421 0.00711506 0.00700375 0.00689954\n",
            " 0.00674686 0.00654717 0.00645786 0.00639408 0.00625039 0.00623934\n",
            " 0.00600695 0.00588633 0.00575062 0.00558899 0.00542309 0.00535541\n",
            " 0.00517289 0.00513963 0.00507159 0.00485733 0.00475006 0.00472919\n",
            " 0.00467537 0.00446327 0.0043806  0.00428298 0.00421754 0.00400303\n",
            " 0.00394998 0.00380737 0.00377468 0.00366926 0.00349378 0.00339799\n",
            " 0.00330336 0.00323617]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "colb7SPAEGqU",
        "colab_type": "code",
        "outputId": "e36e7b43-081b-4463-ae69-dd4ac7f3f8da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sum(pca.explained_variance_ratio_)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9543295537845651"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jk5TpS6QEJuZ",
        "colab_type": "code",
        "outputId": "ac482cd5-f999-4645-8745-f53b86ecf6de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_pca.shape"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8378, 80)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGpG1uV0Eemf",
        "colab_type": "text"
      },
      "source": [
        "# 4- Model Training  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAKjiXM4ZwKB",
        "colab_type": "text"
      },
      "source": [
        "### Train and Test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8zKFaTPEMvM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_pca, df_labels, test_size=0.2, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSym3t8TaH8J",
        "colab_type": "code",
        "outputId": "fc637fa7-8e8e-4a79-c937-9ea5fe180ea3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6702, 80)\n",
            "(1676, 80)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W16opGaJaOkn",
        "colab_type": "code",
        "outputId": "62e8619f-c713-4cc2-ea23-3fb16b7d0d5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6702,)\n",
            "(1676,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOoyFYlkaSyq",
        "colab_type": "code",
        "outputId": "219e058e-1707-4530-dd94-74617aac9486",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "X_train"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-4.68464167, -2.82486563, -1.51036984, ..., -0.04102065,\n",
              "        -0.69027114,  0.2202485 ],\n",
              "       [-0.74018258,  1.56521953, -0.06997607, ...,  0.2491107 ,\n",
              "         0.33756497,  0.42363954],\n",
              "       [ 1.74612928, -3.12552498,  1.71795536, ...,  0.0546167 ,\n",
              "         0.45823286,  0.27855506],\n",
              "       ...,\n",
              "       [ 0.75099825, -2.49960655, -1.51160936, ..., -0.31342012,\n",
              "        -0.21124028,  0.70019733],\n",
              "       [-3.93462881,  2.84871942, -1.62335673, ...,  0.56372732,\n",
              "         0.12162239,  0.08209962],\n",
              "       [-1.56477132, -1.87060749, -0.83035991, ..., -0.45500534,\n",
              "        -0.66363011, -0.23294656]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85lhvFnAaYv1",
        "colab_type": "text"
      },
      "source": [
        "## 4.1)- Manual Neural Network\n",
        "\n",
        "1-Build the Neural Network <br>\n",
        "2 -Set the hyperparameters, train the NN and evaluate<br>\n",
        "3- Adapt SGD method to improve the accuracy<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fglX9k6azn_",
        "colab_type": "text"
      },
      "source": [
        "#### a)-Build the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dnhCoy0ayz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyNeuralNetwork(object):\n",
        "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
        "        # Set number of nodes in input, hidden and output layers.\n",
        "        self.input_nodes =  input_nodes\n",
        "        self.hidden_nodes = hidden_nodes\n",
        "        self.output_nodes = output_nodes\n",
        "\n",
        "        # Initialize weights\n",
        "        self.weights_0_1 = np.zeros((self.hidden_nodes,self.input_nodes))\n",
        "\n",
        "        self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5, \n",
        "                                       (self.output_nodes, self.hidden_nodes))\n",
        "        self.lr = learning_rate\n",
        "        \n",
        "        #### Set this to your implemented sigmoid function ####\n",
        "        # Activation function is the sigmoid function\n",
        "        self.sigmoid_activation = lambda x : 1 / (1 + np.exp(-x))\n",
        "        self.sigmoid_output_2_derivative = lambda x: x * (1 - x)\n",
        "    \n",
        "    def train(self, inputs_array, targets_array):\n",
        "        # Convert inputs list to 2d array\n",
        "        inputs  = inputs_array.T\n",
        "        targets = np.array(targets_array, ndmin=2)\n",
        "        #targets = targets_array\n",
        "        m = inputs_array.shape[0] # number of records\n",
        "        \n",
        "        #### Implement the forward pass here ####\n",
        "        ### Forward pass ###\n",
        "        # TODO: Hidden layer\n",
        "        layer_1_inputs = np.dot(self.weights_0_1, inputs) # signals into hidden layer\n",
        "        layer_1 = layer_1_inputs # signals from hidden layer\n",
        "        \n",
        "        # TODO: Output layer\n",
        "        layer_2_inputs = np.dot(self.weights_1_2,layer_1) # signals into final output layer\n",
        "        layer_2 = self.sigmoid_activation(layer_2_inputs) # signals from final output layer\n",
        "        \n",
        "        #### Implement the backward pass here ####\n",
        "        ### Backward pass ###\n",
        "        \n",
        "        # TODO: Output error  \n",
        "        layer_2_errors = targets - layer_2  # Output layer error is the difference between desired target and actual output.\n",
        "        layer_2_delta = layer_2_errors * self.sigmoid_output_2_derivative(layer_2)\n",
        "        \n",
        "        # TODO: Backpropagated error\n",
        "        layer_1_errors = np.dot(self.weights_1_2.T,layer_2_delta) # errors propagated to the hidden layer 2x128\n",
        "        layer_1_delta = layer_1_errors  # hidden layer gradients y = x -> 1\n",
        "        \n",
        "        # TODO: Update the weights\n",
        "        self.weights_1_2 += self.lr*np.dot(layer_2_delta,layer_1.T)/m # update hidden-to-output weights with gradient descent step\n",
        "        self.weights_0_1 += self.lr*np.dot(layer_1_delta,inputs.T)/m # update input-to-hidden weights with gradient descent step\n",
        "         \n",
        "        \n",
        "    def run(self, inputs_list):\n",
        "        # Run a forward pass through the network\n",
        "        inputs = np.array(inputs_list, ndmin=2).T\n",
        "        \n",
        "        #### Implement the forward pass here ####\n",
        "        # TODO: Hidden layer\n",
        "        hidden_inputs = np.dot(self.weights_0_1, inputs) # signals into hidden layer\n",
        "        hidden_outputs = hidden_inputs # signals from hidden layer\n",
        "        \n",
        "        # TODO: Output layer\n",
        "        final_inputs = np.dot(self.weights_1_2,hidden_outputs) # signals into final output layer\n",
        "        final_outputs = self.sigmoid_activation(final_inputs) # signals from final output layer \n",
        "        \n",
        "        return final_outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIUZWFLwa74e",
        "colab_type": "text"
      },
      "source": [
        "#### b)-Train the model and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWSZQl5Jay_z",
        "colab_type": "code",
        "outputId": "0f797060-aeb5-4def-e802-3c6f4a7b8de0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn import metrics\n",
        "### Set the hyperparameters here ###\n",
        "epochs = 100 #100\n",
        "learning_rate = 0.01 #0.1\n",
        "hidden_nodes = 10 \n",
        "output_nodes = 1\n",
        "\n",
        "N_i = X_train.shape[1]\n",
        "network = MyNeuralNetwork(N_i, hidden_nodes, output_nodes, learning_rate)\n",
        "\n",
        "for e in range(epochs):\n",
        "    network.train(X_train, y_train)\n",
        "    \n",
        "y_pred = network.run(X_test)\n",
        "y_pred = np.where(y_pred >= 0.5, 1, 0) # if probability >= 0.5, it is 1, else 0\n",
        "\n",
        "print(metrics.accuracy_score(y_test,y_pred[0]))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6640811455847255\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWitWEEJcipa",
        "colab_type": "code",
        "outputId": "df7ee3ad-a5d1-4c9f-c21f-6235495c2b91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(metrics.recall_score(y_test,y_pred[0]))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9962825278810409\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ha2kvdFnbDsc",
        "colab_type": "text"
      },
      "source": [
        "#### c)-SGD \n",
        "\n",
        "Stochastic gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDWDvkVCaVp_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_i = X_train.shape[1]\n",
        "network = MyNeuralNetwork(N_i, hidden_nodes, output_nodes, learning_rate)\n",
        "\n",
        "random_row_idx = np.zeros(128)\n",
        "for e in range(epochs):\n",
        "    random_row_idx = np.random.choice(X_train.shape[0],size=128)\n",
        "    X_batch = X_train[random_row_idx,:]\n",
        "    y_batch = y_train[random_row_idx]\n",
        "    network.train(X_batch, y_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo3zc6AgbJpE",
        "colab_type": "code",
        "outputId": "b04b5453-39bd-49d5-f646-34e0dfb1e02e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_pred = network.run(X_test)\n",
        "y_pred = np.where(y_pred >= 0.5, 1, 0) # if probability >= 0.5, it is 1, else 0\n",
        "print(metrics.accuracy_score(y_test,y_pred[0]))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8394988066825776\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SmYT9Tmclkc",
        "colab_type": "text"
      },
      "source": [
        "## 4.2-Neural Network using Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlvGgLKMbNGR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQTnyueacuH6",
        "colab_type": "code",
        "outputId": "8281bcb9-fd9a-4a9d-dfa5-7c4bdde3a824",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1683
        }
      },
      "source": [
        "# Build one layer DNN with 40 units respectively.\n",
        "feature_columns = tf.contrib.learn.infer_real_valued_columns_from_input(X_train)\n",
        "classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns, hidden_units=[40], n_classes=2)\n",
        "\n",
        "# Fit and predict.\n",
        "classifier.fit(X_train, y_train, steps=300)\n",
        "predictions_tf = list(classifier.predict(X_test, as_iterable=True))\n",
        "score = metrics.accuracy_score(y_test, predictions_tf)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0621 21:50:49.835163 139713708164992 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0621 21:50:49.836476 139713708164992 deprecation.py:323] From <ipython-input-49-c9cee6ea5379>:1: infer_real_valued_columns_from_input (from tensorflow.contrib.learn.python.learn.estimators.estimator) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please specify feature columns explicitly.\n",
            "W0621 21:50:49.838068 139713708164992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py:143: setup_train_data_feeder (from tensorflow.contrib.learn.python.learn.learn_io.data_feeder) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tensorflow/transform or tf.data.\n",
            "W0621 21:50:49.843144 139713708164992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py:96: extract_dask_data (from tensorflow.contrib.learn.python.learn.learn_io.dask_io) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please feed input to tf.data to support dask.\n",
            "W0621 21:50:49.847367 139713708164992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py:100: extract_pandas_data (from tensorflow.contrib.learn.python.learn.learn_io.pandas_io) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please access pandas data directly.\n",
            "W0621 21:50:49.848238 139713708164992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py:159: DataFeeder.__init__ (from tensorflow.contrib.learn.python.learn.learn_io.data_feeder) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tensorflow/transform or tf.data.\n",
            "W0621 21:50:49.850626 139713708164992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py:340: check_array (from tensorflow.contrib.learn.python.learn.learn_io.data_feeder) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please convert numpy dtypes explicitly.\n",
            "W0621 21:50:49.852495 139713708164992 data_feeder.py:283] float64 is not supported by many models, consider casting to float32.\n",
            "W0621 21:50:49.855304 139713708164992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py:183: infer_real_valued_columns_from_input_fn (from tensorflow.contrib.learn.python.learn.estimators.estimator) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please specify feature columns explicitly.\n",
            "W0621 21:50:49.860399 139713708164992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:378: multi_class_head (from tensorflow.contrib.learn.python.learn.estimators.head) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.contrib.estimator.*_head.\n",
            "W0621 21:50:49.861425 139713708164992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py:1179: BaseEstimator.__init__ (from tensorflow.contrib.learn.python.learn.estimators.estimator) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please replace uses of any Estimator from tf.contrib.learn with an Estimator from tf.estimator.*\n",
            "W0621 21:50:49.862560 139713708164992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py:427: RunConfig.__init__ (from tensorflow.contrib.learn.python.learn.estimators.run_config) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "When switching to tf.estimator.Estimator, use tf.estimator.RunConfig instead.\n",
            "W0621 21:50:49.864213 139713708164992 estimator.py:453] Using temporary folder as model directory: /tmp/tmp9b9gy1bz\n",
            "W0621 21:50:49.892922 139713708164992 deprecation.py:506] From <ipython-input-49-c9cee6ea5379>:5: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
            "Instructions for updating:\n",
            "Estimator is decoupled from Scikit Learn interface by moving into\n",
            "separate class SKCompat. Arguments x, y and batch_size are only\n",
            "available in the SKCompat class, Estimator will only accept input_fn.\n",
            "Example conversion:\n",
            "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
            "W0621 21:50:49.893646 139713708164992 deprecation.py:506] From <ipython-input-49-c9cee6ea5379>:5: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
            "Instructions for updating:\n",
            "Estimator is decoupled from Scikit Learn interface by moving into\n",
            "separate class SKCompat. Arguments x, y and batch_size are only\n",
            "available in the SKCompat class, Estimator will only accept input_fn.\n",
            "Example conversion:\n",
            "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
            "W0621 21:50:49.894711 139713708164992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py:508: SKCompat.__init__ (from tensorflow.contrib.learn.python.learn.estimators.estimator) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to the Estimator interface.\n",
            "W0621 21:50:49.895896 139713708164992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py:98: extract_dask_labels (from tensorflow.contrib.learn.python.learn.learn_io.dask_io) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please feed input to tf.data to support dask.\n",
            "W0621 21:50:49.896826 139713708164992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py:102: extract_pandas_labels (from tensorflow.contrib.learn.python.learn.learn_io.pandas_io) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please access pandas data directly.\n",
            "W0621 21:50:49.898131 139713708164992 data_feeder.py:283] float64 is not supported by many models, consider casting to float32.\n",
            "W0621 21:50:49.914400 139713708164992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "W0621 21:50:50.381029 139713708164992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0621 21:50:50.463742 139713708164992 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0621 21:50:50.556392 139713708164992 head.py:2027] Casting <dtype: 'int64'> labels to bool.\n",
            "W0621 21:50:50.628074 139713708164992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/metrics_impl.py:809: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "W0621 21:50:50.666106 139713708164992 head.py:2027] Casting <dtype: 'int64'> labels to bool.\n",
            "W0621 21:50:50.735582 139713708164992 metrics_impl.py:804] Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
            "W0621 21:50:50.765990 139713708164992 metrics_impl.py:804] Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
            "W0621 21:50:50.939868 139713708164992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:678: ModelFnOps.__new__ (from tensorflow.contrib.learn.python.learn.estimators.model_fn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "When switching to tf.estimator.Estimator, use tf.estimator.EstimatorSpec. You can use the `estimator_spec` method to create an equivalent one.\n",
            "W0621 21:51:18.632890 139713708164992 deprecation.py:573] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py:574: calling DNNClassifier.predict (from tensorflow.contrib.learn.python.learn.estimators.dnn) with outputs=None is deprecated and will be removed after 2017-03-01.\n",
            "Instructions for updating:\n",
            "Please switch to predict_classes, or set `outputs` argument.\n",
            "W0621 21:51:18.638077 139713708164992 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:463: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
            "Instructions for updating:\n",
            "Estimator is decoupled from Scikit Learn interface by moving into\n",
            "separate class SKCompat. Arguments x, y and batch_size are only\n",
            "available in the SKCompat class, Estimator will only accept input_fn.\n",
            "Example conversion:\n",
            "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
            "W0621 21:51:18.640936 139713708164992 data_feeder.py:283] float64 is not supported by many models, consider casting to float32.\n",
            "W0621 21:51:18.820537 139713708164992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04AmAUBdcy8X",
        "colab_type": "code",
        "outputId": "8620317e-c322-4918-ad37-dc7c181a760f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('TF Accuracy: ', score)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF Accuracy:  0.9976133651551312\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhWw6oGcdCwR",
        "colab_type": "text"
      },
      "source": [
        "# 4.3) Neural network Using Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_4yW7dpc-9T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhLIGFHEc_AL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To evaluate\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PD0X6U0Bc_Dp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "4f02ea76-8f56-4a15-b50c-e7b42d7ace35"
      },
      "source": [
        "#create model, add dense layers one by one specifying activation function\n",
        "model = Sequential()\n",
        "model.add(Dense(15, input_dim=X_train.shape[1], activation='relu')) # input layer requires input_dim param\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dropout(.2))\n",
        "model.add(Dense(1, activation='sigmoid')) # sigmoid instead of relu for final probability between 0 and 1"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0621 21:51:18.917740 139713708164992 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0621 21:51:18.919698 139713708164992 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0621 21:51:18.924459 139713708164992 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0621 21:51:18.969696 139713708164992 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0621 21:51:18.980757 139713708164992 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgkGigGtAe3t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "df81246a-023a-42c8-85f5-747df7d628e2"
      },
      "source": [
        "# compile the model, adam gradient descent (optimized)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0621 21:51:19.011296 139713708164992 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0621 21:51:19.036694 139713708164992 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WcvhtOWAe6z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "2fe3aa08-177c-4946-9382-8638b1fc08d4"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 15)                1215      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                160       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 8)                 88        \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 1,472\n",
            "Trainable params: 1,472\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FMgj-LxAe94",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3394
        },
        "outputId": "4eca1443-8e52-459e-ffbe-aa561391a935"
      },
      "source": [
        "# call the function to fit to the data (training the network)\n",
        "history=model.fit(X_train, y_train, epochs = 100, batch_size=20, validation_data=(X_test, y_test))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 6702 samples, validate on 1676 samples\n",
            "Epoch 1/100\n",
            "6702/6702 [==============================] - 2s 262us/step - loss: 0.3889 - acc: 0.8233 - val_loss: 0.2304 - val_acc: 0.8484\n",
            "Epoch 2/100\n",
            "6702/6702 [==============================] - 1s 201us/step - loss: 0.1251 - acc: 0.9521 - val_loss: 0.0315 - val_acc: 0.9928\n",
            "Epoch 3/100\n",
            "6702/6702 [==============================] - 1s 198us/step - loss: 0.0226 - acc: 0.9952 - val_loss: 0.0077 - val_acc: 0.9994\n",
            "Epoch 4/100\n",
            "6702/6702 [==============================] - 1s 198us/step - loss: 0.0096 - acc: 0.9973 - val_loss: 0.0049 - val_acc: 0.9994\n",
            "Epoch 5/100\n",
            "6702/6702 [==============================] - 1s 196us/step - loss: 0.0046 - acc: 0.9991 - val_loss: 0.0032 - val_acc: 0.9994\n",
            "Epoch 6/100\n",
            "6702/6702 [==============================] - 1s 198us/step - loss: 0.0033 - acc: 0.9996 - val_loss: 0.0034 - val_acc: 0.9994\n",
            "Epoch 7/100\n",
            "6702/6702 [==============================] - 1s 198us/step - loss: 0.0023 - acc: 0.9999 - val_loss: 0.0035 - val_acc: 0.9994\n",
            "Epoch 8/100\n",
            "6702/6702 [==============================] - 1s 200us/step - loss: 0.0020 - acc: 0.9994 - val_loss: 0.0028 - val_acc: 0.9994\n",
            "Epoch 9/100\n",
            "6702/6702 [==============================] - 1s 198us/step - loss: 0.0023 - acc: 0.9996 - val_loss: 0.0019 - val_acc: 0.9994\n",
            "Epoch 10/100\n",
            "6702/6702 [==============================] - 1s 203us/step - loss: 0.0012 - acc: 0.9999 - val_loss: 0.0019 - val_acc: 0.9994\n",
            "Epoch 11/100\n",
            "6702/6702 [==============================] - 1s 197us/step - loss: 0.0016 - acc: 0.9997 - val_loss: 0.0023 - val_acc: 0.9994\n",
            "Epoch 12/100\n",
            "6702/6702 [==============================] - 1s 196us/step - loss: 0.0018 - acc: 0.9997 - val_loss: 0.0025 - val_acc: 0.9994\n",
            "Epoch 13/100\n",
            "6702/6702 [==============================] - 1s 196us/step - loss: 0.0013 - acc: 0.9999 - val_loss: 0.0039 - val_acc: 0.9994\n",
            "Epoch 14/100\n",
            "6702/6702 [==============================] - 1s 199us/step - loss: 0.0021 - acc: 0.9990 - val_loss: 4.3608e-04 - val_acc: 1.0000\n",
            "Epoch 15/100\n",
            "6702/6702 [==============================] - 1s 194us/step - loss: 0.0013 - acc: 0.9997 - val_loss: 0.0040 - val_acc: 0.9988\n",
            "Epoch 16/100\n",
            "6702/6702 [==============================] - 1s 197us/step - loss: 0.0017 - acc: 0.9993 - val_loss: 2.9028e-04 - val_acc: 1.0000\n",
            "Epoch 17/100\n",
            "6702/6702 [==============================] - 1s 194us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 3.0349e-04 - val_acc: 1.0000\n",
            "Epoch 18/100\n",
            "6702/6702 [==============================] - 1s 197us/step - loss: 0.0011 - acc: 0.9997 - val_loss: 6.5723e-05 - val_acc: 1.0000\n",
            "Epoch 19/100\n",
            "6702/6702 [==============================] - 1s 197us/step - loss: 0.0014 - acc: 0.9999 - val_loss: 9.9352e-05 - val_acc: 1.0000\n",
            "Epoch 20/100\n",
            "6702/6702 [==============================] - 1s 201us/step - loss: 6.1603e-04 - acc: 0.9999 - val_loss: 4.8130e-05 - val_acc: 1.0000\n",
            "Epoch 21/100\n",
            "6702/6702 [==============================] - 1s 196us/step - loss: 4.7534e-04 - acc: 1.0000 - val_loss: 5.5370e-05 - val_acc: 1.0000\n",
            "Epoch 22/100\n",
            "6702/6702 [==============================] - 1s 196us/step - loss: 0.0010 - acc: 0.9999 - val_loss: 1.1722e-04 - val_acc: 1.0000\n",
            "Epoch 23/100\n",
            "6702/6702 [==============================] - 1s 197us/step - loss: 8.5778e-04 - acc: 0.9997 - val_loss: 1.0320e-04 - val_acc: 1.0000\n",
            "Epoch 24/100\n",
            "6702/6702 [==============================] - 1s 195us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 1.7607e-04 - val_acc: 1.0000\n",
            "Epoch 25/100\n",
            "6702/6702 [==============================] - 1s 198us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 0.0088 - val_acc: 0.9988\n",
            "Epoch 26/100\n",
            "6702/6702 [==============================] - 1s 204us/step - loss: 0.0037 - acc: 0.9993 - val_loss: 0.0061 - val_acc: 0.9982\n",
            "Epoch 27/100\n",
            "6702/6702 [==============================] - 1s 198us/step - loss: 0.0013 - acc: 0.9999 - val_loss: 0.0092 - val_acc: 0.9988\n",
            "Epoch 28/100\n",
            "6702/6702 [==============================] - 1s 198us/step - loss: 7.3644e-04 - acc: 1.0000 - val_loss: 0.0098 - val_acc: 0.9988\n",
            "Epoch 29/100\n",
            "6702/6702 [==============================] - 1s 200us/step - loss: 0.0010 - acc: 0.9997 - val_loss: 0.0095 - val_acc: 0.9994\n",
            "Epoch 30/100\n",
            "6702/6702 [==============================] - 1s 205us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0093 - val_acc: 0.9994\n",
            "Epoch 31/100\n",
            "6702/6702 [==============================] - 1s 198us/step - loss: 5.2509e-04 - acc: 0.9997 - val_loss: 0.0070 - val_acc: 0.9994\n",
            "Epoch 32/100\n",
            "6702/6702 [==============================] - 1s 197us/step - loss: 7.7427e-04 - acc: 0.9999 - val_loss: 0.0070 - val_acc: 0.9994\n",
            "Epoch 33/100\n",
            "6702/6702 [==============================] - 1s 202us/step - loss: 3.4707e-04 - acc: 1.0000 - val_loss: 0.0067 - val_acc: 0.9994\n",
            "Epoch 34/100\n",
            "6702/6702 [==============================] - 1s 199us/step - loss: 7.2606e-04 - acc: 0.9997 - val_loss: 0.0066 - val_acc: 0.9994\n",
            "Epoch 35/100\n",
            "6702/6702 [==============================] - 1s 194us/step - loss: 3.7442e-04 - acc: 0.9999 - val_loss: 0.0063 - val_acc: 0.9994\n",
            "Epoch 36/100\n",
            "6702/6702 [==============================] - 1s 197us/step - loss: 3.7433e-04 - acc: 0.9999 - val_loss: 0.0076 - val_acc: 0.9988\n",
            "Epoch 37/100\n",
            "6702/6702 [==============================] - 1s 195us/step - loss: 7.2634e-04 - acc: 0.9999 - val_loss: 0.0072 - val_acc: 0.9988\n",
            "Epoch 38/100\n",
            "6702/6702 [==============================] - 1s 198us/step - loss: 8.8567e-04 - acc: 0.9997 - val_loss: 0.0069 - val_acc: 0.9988\n",
            "Epoch 39/100\n",
            "6702/6702 [==============================] - 1s 196us/step - loss: 5.3263e-04 - acc: 0.9999 - val_loss: 0.0069 - val_acc: 0.9988\n",
            "Epoch 40/100\n",
            "6702/6702 [==============================] - 1s 198us/step - loss: 4.1672e-04 - acc: 1.0000 - val_loss: 0.0064 - val_acc: 0.9988\n",
            "Epoch 41/100\n",
            "6702/6702 [==============================] - 1s 195us/step - loss: 7.3774e-04 - acc: 0.9999 - val_loss: 0.0044 - val_acc: 0.9988\n",
            "Epoch 42/100\n",
            "6702/6702 [==============================] - 1s 193us/step - loss: 5.6392e-04 - acc: 0.9999 - val_loss: 0.0050 - val_acc: 0.9988\n",
            "Epoch 43/100\n",
            "6702/6702 [==============================] - 1s 193us/step - loss: 3.7242e-04 - acc: 1.0000 - val_loss: 0.0046 - val_acc: 0.9988\n",
            "Epoch 44/100\n",
            "6702/6702 [==============================] - 1s 196us/step - loss: 8.0538e-04 - acc: 0.9999 - val_loss: 0.0042 - val_acc: 0.9988\n",
            "Epoch 45/100\n",
            "6702/6702 [==============================] - 1s 195us/step - loss: 7.2990e-04 - acc: 0.9999 - val_loss: 0.0041 - val_acc: 0.9988\n",
            "Epoch 46/100\n",
            "6702/6702 [==============================] - 1s 194us/step - loss: 0.0014 - acc: 0.9994 - val_loss: 0.0026 - val_acc: 0.9994\n",
            "Epoch 47/100\n",
            "6702/6702 [==============================] - 1s 195us/step - loss: 3.2877e-04 - acc: 1.0000 - val_loss: 0.0025 - val_acc: 0.9994\n",
            "Epoch 48/100\n",
            "6702/6702 [==============================] - 1s 196us/step - loss: 9.2364e-04 - acc: 0.9996 - val_loss: 0.0024 - val_acc: 0.9994\n",
            "Epoch 49/100\n",
            "6702/6702 [==============================] - 1s 197us/step - loss: 4.7848e-04 - acc: 0.9999 - val_loss: 0.0022 - val_acc: 0.9994\n",
            "Epoch 50/100\n",
            "6702/6702 [==============================] - 1s 201us/step - loss: 2.9689e-04 - acc: 0.9999 - val_loss: 0.0053 - val_acc: 0.9994\n",
            "Epoch 51/100\n",
            "6702/6702 [==============================] - 1s 194us/step - loss: 5.1986e-04 - acc: 0.9999 - val_loss: 0.0050 - val_acc: 0.9994\n",
            "Epoch 52/100\n",
            "6702/6702 [==============================] - 1s 192us/step - loss: 4.7464e-04 - acc: 0.9999 - val_loss: 0.0049 - val_acc: 0.9994\n",
            "Epoch 53/100\n",
            "6702/6702 [==============================] - 1s 196us/step - loss: 0.0016 - acc: 0.9993 - val_loss: 0.0036 - val_acc: 0.9994\n",
            "Epoch 54/100\n",
            "6702/6702 [==============================] - 1s 198us/step - loss: 8.7622e-04 - acc: 0.9996 - val_loss: 0.0030 - val_acc: 0.9994\n",
            "Epoch 55/100\n",
            "6702/6702 [==============================] - 1s 192us/step - loss: 8.5405e-04 - acc: 0.9997 - val_loss: 0.0029 - val_acc: 0.9994\n",
            "Epoch 56/100\n",
            "6702/6702 [==============================] - 1s 196us/step - loss: 0.0100 - acc: 0.9990 - val_loss: 0.0133 - val_acc: 0.9976\n",
            "Epoch 57/100\n",
            "6702/6702 [==============================] - 1s 198us/step - loss: 0.0042 - acc: 0.9987 - val_loss: 0.0090 - val_acc: 0.9988\n",
            "Epoch 58/100\n",
            "6702/6702 [==============================] - 1s 193us/step - loss: 0.0011 - acc: 0.9996 - val_loss: 0.0100 - val_acc: 0.9988\n",
            "Epoch 59/100\n",
            "6702/6702 [==============================] - 1s 195us/step - loss: 7.3441e-04 - acc: 0.9997 - val_loss: 0.0079 - val_acc: 0.9988\n",
            "Epoch 60/100\n",
            "6702/6702 [==============================] - 1s 198us/step - loss: 9.3526e-04 - acc: 0.9997 - val_loss: 0.0079 - val_acc: 0.9988\n",
            "Epoch 61/100\n",
            "6702/6702 [==============================] - 1s 199us/step - loss: 8.4829e-04 - acc: 0.9996 - val_loss: 0.0078 - val_acc: 0.9988\n",
            "Epoch 62/100\n",
            "6702/6702 [==============================] - 1s 197us/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.0070 - val_acc: 0.9988\n",
            "Epoch 63/100\n",
            "6702/6702 [==============================] - 1s 203us/step - loss: 6.8362e-04 - acc: 0.9999 - val_loss: 0.0075 - val_acc: 0.9988\n",
            "Epoch 64/100\n",
            "6702/6702 [==============================] - 1s 199us/step - loss: 3.7392e-04 - acc: 1.0000 - val_loss: 0.0074 - val_acc: 0.9994\n",
            "Epoch 65/100\n",
            "6702/6702 [==============================] - 1s 199us/step - loss: 0.0014 - acc: 0.9994 - val_loss: 0.0074 - val_acc: 0.9994\n",
            "Epoch 66/100\n",
            "6702/6702 [==============================] - 1s 195us/step - loss: 9.9659e-04 - acc: 0.9996 - val_loss: 0.0061 - val_acc: 0.9988\n",
            "Epoch 67/100\n",
            "6702/6702 [==============================] - 1s 198us/step - loss: 7.5718e-04 - acc: 0.9997 - val_loss: 0.0060 - val_acc: 0.9988\n",
            "Epoch 68/100\n",
            "6702/6702 [==============================] - 1s 196us/step - loss: 2.8166e-04 - acc: 1.0000 - val_loss: 0.0059 - val_acc: 0.9988\n",
            "Epoch 69/100\n",
            "6702/6702 [==============================] - 1s 193us/step - loss: 8.0922e-04 - acc: 0.9999 - val_loss: 0.0055 - val_acc: 0.9994\n",
            "Epoch 70/100\n",
            "6702/6702 [==============================] - 1s 194us/step - loss: 2.3053e-04 - acc: 1.0000 - val_loss: 0.0053 - val_acc: 0.9994\n",
            "Epoch 71/100\n",
            "6702/6702 [==============================] - 1s 200us/step - loss: 6.4140e-04 - acc: 0.9997 - val_loss: 0.0053 - val_acc: 0.9994\n",
            "Epoch 72/100\n",
            "6702/6702 [==============================] - 1s 196us/step - loss: 3.7864e-04 - acc: 0.9999 - val_loss: 0.0048 - val_acc: 0.9994\n",
            "Epoch 73/100\n",
            "6702/6702 [==============================] - 1s 196us/step - loss: 6.3326e-04 - acc: 0.9999 - val_loss: 0.0047 - val_acc: 0.9994\n",
            "Epoch 74/100\n",
            "6702/6702 [==============================] - 1s 196us/step - loss: 5.7120e-04 - acc: 0.9999 - val_loss: 0.0027 - val_acc: 0.9994\n",
            "Epoch 75/100\n",
            "6702/6702 [==============================] - 1s 198us/step - loss: 7.8608e-04 - acc: 0.9997 - val_loss: 0.0030 - val_acc: 0.9994\n",
            "Epoch 76/100\n",
            "6702/6702 [==============================] - 1s 195us/step - loss: 8.9072e-04 - acc: 0.9997 - val_loss: 0.0028 - val_acc: 0.9994\n",
            "Epoch 77/100\n",
            "6702/6702 [==============================] - 1s 193us/step - loss: 7.5447e-04 - acc: 0.9999 - val_loss: 0.0028 - val_acc: 0.9994\n",
            "Epoch 78/100\n",
            "6702/6702 [==============================] - 1s 200us/step - loss: 7.1679e-04 - acc: 0.9997 - val_loss: 0.0030 - val_acc: 0.9994\n",
            "Epoch 79/100\n",
            "6702/6702 [==============================] - 1s 198us/step - loss: 2.3618e-04 - acc: 1.0000 - val_loss: 0.0027 - val_acc: 0.9994\n",
            "Epoch 80/100\n",
            "6702/6702 [==============================] - 1s 206us/step - loss: 0.0015 - acc: 0.9993 - val_loss: 0.0026 - val_acc: 0.9994\n",
            "Epoch 81/100\n",
            "6702/6702 [==============================] - 1s 200us/step - loss: 5.4628e-04 - acc: 0.9999 - val_loss: 0.0032 - val_acc: 0.9988\n",
            "Epoch 82/100\n",
            "6702/6702 [==============================] - 1s 198us/step - loss: 0.0010 - acc: 0.9996 - val_loss: 0.0019 - val_acc: 0.9988\n",
            "Epoch 83/100\n",
            "6702/6702 [==============================] - 1s 198us/step - loss: 7.8654e-04 - acc: 0.9997 - val_loss: 7.5624e-04 - val_acc: 0.9994\n",
            "Epoch 84/100\n",
            "6702/6702 [==============================] - 1s 196us/step - loss: 4.0077e-04 - acc: 1.0000 - val_loss: 8.9093e-05 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "6702/6702 [==============================] - 1s 198us/step - loss: 4.4088e-04 - acc: 0.9999 - val_loss: 7.3419e-05 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "6702/6702 [==============================] - 1s 198us/step - loss: 0.0012 - acc: 0.9994 - val_loss: 2.2971e-05 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "6702/6702 [==============================] - 1s 198us/step - loss: 0.0011 - acc: 0.9996 - val_loss: 2.0665e-05 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "6702/6702 [==============================] - 1s 203us/step - loss: 5.7283e-04 - acc: 0.9999 - val_loss: 2.0600e-05 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "6702/6702 [==============================] - 1s 195us/step - loss: 5.3757e-04 - acc: 0.9999 - val_loss: 1.8973e-05 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "6702/6702 [==============================] - 1s 199us/step - loss: 3.5144e-04 - acc: 1.0000 - val_loss: 1.2774e-05 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "6702/6702 [==============================] - 1s 200us/step - loss: 5.9313e-04 - acc: 0.9999 - val_loss: 1.4986e-05 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "6702/6702 [==============================] - 1s 200us/step - loss: 2.4384e-04 - acc: 1.0000 - val_loss: 4.7392e-06 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "6702/6702 [==============================] - 1s 202us/step - loss: 0.0033 - acc: 0.9996 - val_loss: 0.0225 - val_acc: 0.9976\n",
            "Epoch 94/100\n",
            "6702/6702 [==============================] - 1s 202us/step - loss: 0.0031 - acc: 0.9996 - val_loss: 0.0290 - val_acc: 0.9982\n",
            "Epoch 95/100\n",
            "6702/6702 [==============================] - 1s 203us/step - loss: 0.0029 - acc: 0.9997 - val_loss: 0.0289 - val_acc: 0.9982\n",
            "Epoch 96/100\n",
            "6702/6702 [==============================] - 1s 197us/step - loss: 0.0038 - acc: 0.9993 - val_loss: 0.0289 - val_acc: 0.9982\n",
            "Epoch 97/100\n",
            "6702/6702 [==============================] - 1s 197us/step - loss: 0.0030 - acc: 0.9997 - val_loss: 0.0289 - val_acc: 0.9982\n",
            "Epoch 98/100\n",
            "6702/6702 [==============================] - 1s 199us/step - loss: 0.0033 - acc: 0.9994 - val_loss: 0.0289 - val_acc: 0.9982\n",
            "Epoch 99/100\n",
            "6702/6702 [==============================] - 1s 199us/step - loss: 0.0028 - acc: 0.9997 - val_loss: 0.0125 - val_acc: 0.9988\n",
            "Epoch 100/100\n",
            "6702/6702 [==============================] - 1s 197us/step - loss: 0.0038 - acc: 0.9993 - val_loss: 0.0133 - val_acc: 0.9988\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiLOwgHEAfA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred_keras = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnezHaNPc_Gb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#convert in crisp class\n",
        "yhat_class_keras = model.predict_classes(X_test, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvEQDCZkCOB4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "ddc65825-e5fb-41be-fab2-ca8d6be7713a"
      },
      "source": [
        "yhat_class_keras[:5]"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4rcBhAtA8H8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "28e2aef1-9dd7-40e5-d258-c56446e98947"
      },
      "source": [
        "print(recall_score(y_test, yhat_class_keras))\n",
        "print(accuracy_score(y_test, yhat_class_keras))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9925650557620818\n",
            "0.9988066825775657\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Bgybz8IA8Qt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "0f08a3a7-5eef-4cf0-de62-0d051ecc1eb9"
      },
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "print(classification_report(y_test, yhat_class_keras))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      1407\n",
            "           1       1.00      0.99      1.00       269\n",
            "\n",
            "    accuracy                           1.00      1676\n",
            "   macro avg       1.00      1.00      1.00      1676\n",
            "weighted avg       1.00      1.00      1.00      1676\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YL5T1xzYA8XM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "def plot_history(history):\n",
        "    acc = history.history['acc']\n",
        "    val_acc = history.history['val_acc']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, acc, 'b', label='Training acc')\n",
        "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrnJK2vxBDz_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "4d87482d-8da3-4b8b-f942-11f3b1718ed7"
      },
      "source": [
        "plot_history(history)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtMAAAFACAYAAAB+7vBBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLoHF99Yc-R1",
        "colab_type": "text"
      },
      "source": [
        "## 4.4)- SVC\n",
        "\n",
        "It is same as SVM. is a wrapper around the libsvm library and supports different kernels while LinearSVC is based on liblinear and only supports a linear kernel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6AbdH5Sc4a6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "svc = SVC()\n",
        "svc.fit(X_train, y_train)\n",
        "y_pred_svc = svc.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4uTSWpAdQbH",
        "colab_type": "code",
        "outputId": "dbf12979-fdc3-49a4-ab96-0e826e7123f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "y_pred_svc[:30]"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nut-OASVdVgi",
        "colab_type": "code",
        "outputId": "598d437e-20a9-4c15-cc31-bc5db17a1536",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('SVM Accuracy: ', metrics.accuracy_score(y_test,y_pred_svc))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM Accuracy:  0.9946300715990454\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YnobxdudvuJ",
        "colab_type": "code",
        "outputId": "23ad36f6-8eaf-4e6e-e7d3-4e382d4808e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(metrics.recall_score(y_test,y_pred_svc))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.966542750929368\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JC8Gb8I-dlps",
        "colab_type": "text"
      },
      "source": [
        "## 4.5)-Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6lX-jbGdYrJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logreg = LogisticRegression(C=1e5)\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred_log = logreg.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHorxx-xdqXA",
        "colab_type": "code",
        "outputId": "166ecc80-e173-4b4e-f246-b41eb0cba296",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_pred_log"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ2cu1yEdw3O",
        "colab_type": "code",
        "outputId": "45ef03c6-0bc5-4e93-e811-eba8f18e77b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Log Regression Accuracy: ', metrics.accuracy_score(y_test,y_pred_log))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Log Regression Accuracy:  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLPwPjZgdmN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKGdsOz9e4V8",
        "colab_type": "text"
      },
      "source": [
        "Seems overfit as accuracy can only be 100% in an ideal scenario"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfvPkq88dnUD",
        "colab_type": "code",
        "outputId": "7268b2f3-df2f-4f60-c1a5-65783d4891c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(metrics.recall_score(y_test,y_pred_log))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_i_w-xueNoq",
        "colab_type": "text"
      },
      "source": [
        "Manual NN: 0.839498806683.<br>\n",
        "TF Accuracy: 0.999403341289<br>\n",
        "Dense Neural Network:0.998 <br>\n",
        "SVM Accuracy: 0.994630071599<br>\n",
        "Log Regression Accuracy: 1.0<br>"
      ]
    }
  ]
}